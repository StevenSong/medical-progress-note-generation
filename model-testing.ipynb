{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "from models import (\n",
    "    Classifier,\n",
    "    ClassifierConfig,\n",
    "    ClinicalEncoder,\n",
    "    ClinicalEncoderConfig,\n",
    "    TextEncoder,\n",
    "    TextEncoderConfig,\n",
    "    MultimodalEncoder,\n",
    "    MultimodalConfig,\n",
    "    freeze_model,\n",
    "    unfreeze_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cln_enc_cfg = ClinicalEncoderConfig(\n",
    "    feature_names=['HR', 'RR', 'SBP', 'DBP', 'SpO2', 'GCS'],\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=3072,\n",
    "    hidden_act=\"gelu\",\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=512,\n",
    "    initializer_range=0.02,\n",
    "    type_vocab_size=1, # no multi-sentence type for clinical encoder\n",
    ")\n",
    "\n",
    "# txt_enc_cfg = TextEncoderConfig(\n",
    "#     vocab_size=30522,\n",
    "#     hidden_size=768,\n",
    "#     num_hidden_layers=12,\n",
    "#     num_attention_heads=12,\n",
    "#     intermediate_size=3072,\n",
    "#     hidden_act=\"gelu\",\n",
    "#     hidden_dropout_prob=0.1,\n",
    "#     attention_probs_dropout_prob=0.1,\n",
    "#     max_position_embeddings=512,\n",
    "#     initializer_range=0.02,\n",
    "#     type_vocab_size=1, # no multi-sentence type for clinical encoder\n",
    "# )\n",
    "\n",
    "mm_enc_cfg = MultimodalConfig(\n",
    "    hidden_size=768,\n",
    "    initializer_range=0.02,\n",
    "    cln_enc_cfg=cln_enc_cfg,\n",
    "    # txt_enc_cfg=txt_enc_cfg,\n",
    ")\n",
    "\n",
    "cls_cfg = ClassifierConfig(\n",
    "    num_labels=2,\n",
    "    hidden_size=768,\n",
    "    initializer_range=0.02,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = pd.read_csv('/mnt/data1/mimic/iii/aligned/all_records.csv')\n",
    "batch_records = all_records.sample(n=64, random_state=42)\n",
    "clinical_features = []\n",
    "notes = []\n",
    "for i, row in tqdm(batch_records.iterrows()):\n",
    "    subj = row['SUBJECT_ID']\n",
    "    hadm = row['HADM_ID']\n",
    "    note = row['NOTE_NUM']\n",
    "    x = pd.read_csv(f'/mnt/data1/mimic/iii/aligned/feats/{subj}-{hadm}-{note}.csv')[cln_enc_cfg.feature_names].to_numpy()\n",
    "    note = pd.read_csv(f'/mnt/data1/mimic/iii/aligned/notes/{subj}-{hadm}-{note}.csv').iloc[0, 0]\n",
    "    clinical_features.append(x)\n",
    "    notes.append(note)\n",
    "clinical_features = np.stack(clinical_features)\n",
    "mortality = batch_records['60D_MORTALITY'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpath = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "tok = AutoTokenizer.from_pretrained(mpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = MultimodalEncoder(mm_enc_cfg)\n",
    "enc.text_from_pretrained(mpath)\n",
    "cls = Classifier(cls_cfg)\n",
    "\n",
    "enc.to(\"cuda\")\n",
    "cls.to(\"cuda\")\n",
    "clinical_features = torch.as_tensor(clinical_features, dtype=torch.float32, device='cuda')\n",
    "mortality = torch.as_tensor(mortality, device='cuda')\n",
    "notes_tok = tok(notes, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(\"cuda\")\n",
    "\n",
    "opt = torch.optim.Adam(nn.ModuleList([enc, cls]).parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    enc_out = enc(\n",
    "        clinical_features=clinical_features,\n",
    "        input_ids=notes_tok[\"input_ids\"],\n",
    "        attention_mask=notes_tok[\"attention_mask\"],\n",
    "    )\n",
    "    logits = cls(enc_out)\n",
    "    loss = loss_fn(logits, mortality)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    print(loss.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(X, y):\n",
    "#     cln_model = ClinicalEncoder(cln_enc_cfg, feat_names)\n",
    "#     opt = torch.optim.Adam(cln_model.parameters(), lr=0.01)\n",
    "#     loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medpng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
